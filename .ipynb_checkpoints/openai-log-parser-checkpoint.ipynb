{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "2683a76e-4c1e-4410-8550-a1dc484b1a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Library import\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import re\n",
    "import faiss\n",
    "import textwrap\n",
    "from tqdm import tqdm\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0798a79e-5ba0-41df-aac6-491863447695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some utils\n",
    "def create_generic_template(template: str) -> str:\n",
    "    return re.sub(r\"<[^>]+>\", \"<*>\", template.strip())\n",
    "\n",
    "def build_regex_from_template(template: str) -> str:\n",
    "    regex_parts = []\n",
    "    pos = 0\n",
    "    while pos < len(template):\n",
    "        if template[pos] == \"<\":\n",
    "            end = template.find(\">\", pos)\n",
    "            if end == -1:\n",
    "                raise ValueError(\"Unmatched '<' in template\")\n",
    "            var_name = template[pos+1:end]\n",
    "            # Generic field match: match non-space sequences\n",
    "            regex_parts.append(f\"(?P<{var_name}>\\\\S+)\")\n",
    "            pos = end + 1\n",
    "        else:\n",
    "            # Treat spaces as \\s+, other chars escaped\n",
    "            if template[pos].isspace():\n",
    "                regex_parts.append(r\"\\s+\")\n",
    "            else:\n",
    "                regex_parts.append(re.escape(template[pos]))\n",
    "            pos += 1\n",
    "    return \"^\" + \"\".join(regex_parts) + \"$\"\n",
    "\n",
    "def extract_fields(log_line: str, template: str) -> dict:\n",
    "    pattern = build_regex_from_template(template)\n",
    "    match = re.match(pattern, log_line)\n",
    "    if match:\n",
    "        return match.groupdict()\n",
    "    else:\n",
    "        return {}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c68b3e8-e4ed-4a41-bebf-95c63c0b4ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare gold for HDFS data\n",
    "\n",
    "def create_annotated_template_hdfs(row):\n",
    "    prefix = \"<DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: \"\n",
    "    content = \"\"\n",
    "    if row[\"EventId\"] == \"E1\":\n",
    "        content = \"<IP_ADDRESS_1>:<PORT> Served block blk_<BLOCK_ID> to /<IP_ADDRESS_2>\"\n",
    "    elif row[\"EventId\"] == \"E2\":\n",
    "        content = \"<IP_ADDRESS_1>:<PORT_1> Starting thread to transfer block blk_<BLOCK_ID> to <IP_ADDRESS_2>:<PORT_2>\"\n",
    "    elif row[\"EventId\"] == \"E3\":\n",
    "        content = \"<IP_ADDRESS_1>:<PORT>:Got exception while serving blk_<BLOCK_ID> to /<IP_ADDRESS_2>:\"\n",
    "    elif row[\"EventId\"] == \"E4\":\n",
    "        content = \"BLOCK* ask <IP_ADDRESS>:<PORT> to delete  blk_<BLOCK_ID>\"\n",
    "    elif row[\"EventId\"] == \"E5\":\n",
    "        content = \"BLOCK* ask <IP_ADDRESS_1>:<PORT_1> to replicate blk_<BLOCK_ID> to datanode(s) <IP_ADDRESS_2>:<PORT_2>\"\n",
    "    elif row[\"EventId\"] == \"E6\":\n",
    "        content = \"BLOCK* NameSystem.addStoredBlock: blockMap updated: <IP_ADDRESS>:<PORT> is added to blk_<BLOCK_ID> size <SIZE>\"\n",
    "    elif row[\"EventId\"] == \"E7\":\n",
    "        content = \"BLOCK* NameSystem.allocateBlock: /<FILE_PATH>/part-<PART_ID>. blk_<BLOCK_ID>\"\n",
    "    elif row[\"EventId\"] == \"E8\":\n",
    "        content = \"BLOCK* NameSystem.delete: blk_<BLOCK_ID> is added to invalidSet of <IP_ADDRESS>:<PORT>\"\n",
    "    elif row[\"EventId\"] == \"E9\":\n",
    "        content = \"Deleting block blk_<BLOCK_ID_1> file /<FILE_PATH>/blk_<BLOCK_ID_2>\"\n",
    "    elif row[\"EventId\"] == \"E10\":\n",
    "        content = \"PacketResponder <PACKET_RESPONDER_NUM> for block blk_<BLOCK_ID> terminating\"\n",
    "    elif row[\"EventId\"] == \"E11\":\n",
    "        content = \"Received block blk_<BLOCK_ID> of size <SIZE> from /<IP_ADDRESS>\"\n",
    "    elif row[\"EventId\"] == \"E12\":\n",
    "        content = \"Received block blk_<BLOCK_ID> src: /<SRC_IP_ADDRESS>:<SRC_PORT> dest: /<DEST_IP_ADDRESS>:<DEST_PORT> of size <SIZE>\"\n",
    "    elif row[\"EventId\"] == \"E13\":\n",
    "        content = \"Receiving block blk_<BLOCK_ID> src: /<SRC_IP_ADDRESS>:<SRC_PORT> dest: /<DEST_IP_ADDRESS>:<DEST_PORT>\"\n",
    "    elif row[\"EventId\"] == \"E14\":\n",
    "        content = \"Verification succeeded for blk_<BLOCK_ID>\"\n",
    "    return f\"{prefix}{content}\"\n",
    "\n",
    "\n",
    "def get_gold():\n",
    "    PATH = \"data/loghub_2k/HDFS/HDFS_2k.log\"\n",
    "    # HDFS_LOG_STRUCTURED = \"data/loghub_2k/HDFS/HDFS_2k.log_structured.csv\"\n",
    "    # hls = pd.read_csv(HDFS_LOG_STRUCTURED)\n",
    "    HDFS_RAW = \"HDFS_2k.log_structured.csv\"\n",
    "    hls = pd.read_csv(HDFS_RAW)\n",
    "    hdfs_gold = hls[[\"LineId\", \"EventId\"]]\n",
    "    hdfs_gold[\"OriginalLog\"] = hls.apply(lambda row: f\"{row[\"Date\"]} {row[\"Time\"]} {row[\"Pid\"]} {row[\"Level\"]} {row[\"Component\"]}: {row[\"Content\"]}\", axis = 1)\n",
    "    hdfs_gold[\"TemplateGeneric\"] = hls.apply(lambda row: f\"<*> <*> <*> <*> <*>: {row[\"EventTemplate\"]}\", axis = 1)\n",
    "    hdfs_gold[\"TemplateAnnotated\"] = hls.apply(create_annotated_template_hdfs, axis = 1)\n",
    "    hdfs_gold[\"VariablesJson\"] = hdfs_gold.apply(lambda row: json.dumps(extract_fields(row[\"OriginalLog\"], row[\"TemplateAnnotated\"])), axis = 1)\n",
    "    return hdfs_gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "73af9f0f-98dd-428a-955b-861b2d640bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQLite for storing candidate extracted logs and embeddings\n",
    "\n",
    "DBNAME = \"parsedlog.db\"\n",
    "CREATE_TABLE_SQL = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS log_examples (\n",
    "    id INTEGER,\n",
    "\tlog_line TEXT NOT NULL,\n",
    "\ttemplate_generic TEXT NOT NULL,\n",
    "\ttemplate_annotated TEXT NOT NULL,\n",
    "\tvariables_json\tTEXT NOT NULL,\n",
    "    embedding BLOB NOT NULL,\n",
    "\tPRIMARY KEY(id AUTOINCREMENT)\n",
    ")\n",
    "\"\"\"\n",
    "conn = sqlite3.connect(DBNAME)\n",
    "c = conn.cursor()\n",
    "c.execute(CREATE_TABLE_SQL)\n",
    "conn.commit()\n",
    "\n",
    "def insert_log_example(log_line: str, template_annotated: str, variables: dict, emb, connection):\n",
    "    # emb = get_normalized_embedding(log_line)\n",
    "    emb_blob = emb.tobytes()\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(\"\"\"\n",
    "    INSERT INTO log_examples(log_line, template_generic, template_annotated, variables_json, embedding)\n",
    "    VALUES(?, ?, ?, ?, ?)\n",
    "    \"\"\", (log_line,  create_generic_template(template_annotated) ,template_annotated, json.dumps(variables), emb_blob))\n",
    "    connection.commit()\n",
    "\n",
    "# Embedding and top-k\n",
    "\n",
    "class LogRetriever:\n",
    "    def __init__(self, db_path, mode=\"full\"):\n",
    "        assert mode in (\"partial\", \"full\"), \"mode must be 'partial' or 'full'\"\n",
    "        self.mode = mode\n",
    "        self.conn = sqlite3.connect(db_path)\n",
    "        self.client = OpenAI()\n",
    "        self.c = self.conn.cursor()\n",
    "\n",
    "    def truncate_log_examples_db(self):\n",
    "        self.c.execute(\"DELETE FROM log_examples\")\n",
    "        self.c.execute(\"DELETE FROM sqlite_sequence WHERE name='log_examples'\")\n",
    "        self.conn.commit()\n",
    "\n",
    "    def close_conn(self):\n",
    "        self.conn.close()\n",
    "\n",
    "    def load_embeddings(self):\n",
    "        if self.mode == \"partial\":\n",
    "            self._load_partial()\n",
    "        else:\n",
    "            self._load_full()\n",
    "\n",
    "    def _load_partial(self):\n",
    "        self.c.execute(\"SELECT id, embedding FROM log_examples\")\n",
    "        rows = self.c.fetchall()\n",
    "\n",
    "        self.db_ids = []\n",
    "        embeddings = []\n",
    "\n",
    "        for db_id, emb_blob in rows:\n",
    "            emb = np.frombuffer(emb_blob, dtype=np.float32)\n",
    "            embeddings.append(emb)\n",
    "            self.db_ids.append(db_id)\n",
    "\n",
    "        if embeddings:\n",
    "            embeddings_np = np.array(embeddings).astype('float32')\n",
    "            dim = embeddings_np.shape[1]\n",
    "            self.index = faiss.IndexFlatIP(dim)\n",
    "            self.index.add(embeddings_np)\n",
    "            self.faiss_id_to_db_id = {i: self.db_ids[i] for i in range(len(self.db_ids))}\n",
    "        else:\n",
    "            dim = 1536  # default for 'text-embedding-ada-002'\n",
    "            print(\"No data found in DB. Initializing empty FAISS index.\")\n",
    "            self.index = faiss.IndexFlatIP(dim)\n",
    "            self.faiss_id_to_db_id = {}\n",
    "\n",
    "    def _load_full(self):\n",
    "        self.c.execute(\"\"\"\n",
    "        SELECT id, log_line, template_generic, template_annotated, variables_json, embedding\n",
    "        FROM log_examples\n",
    "        \"\"\")\n",
    "        rows = self.c.fetchall()\n",
    "\n",
    "        self.db_ids = []\n",
    "        embeddings = []\n",
    "        self.db_records = {} # db_id -> record dict\n",
    "\n",
    "        for db_id, log_line, template_generic, template_annotated, variables_json, emb_blob in rows:\n",
    "            emb = np.frombuffer(emb_blob, dtype=np.float32)\n",
    "            embeddings.append(emb)\n",
    "            self.db_ids.append(db_id)\n",
    "            self.db_records[db_id] = {\n",
    "                \"id\": db_id,\n",
    "                \"original_log\": log_line,\n",
    "                \"template_generic\": template_generic,\n",
    "                \"template_annotated\": template_annotated,\n",
    "                \"variables_json\": variables_json\n",
    "            }\n",
    "\n",
    "        if embeddings:\n",
    "            embeddings_np = np.array(embeddings).astype('float32')\n",
    "            dim = embeddings_np.shape[1]\n",
    "            self.index = faiss.IndexFlatIP(dim)\n",
    "            self.index.add(embeddings_np)\n",
    "            self.faiss_id_to_db_id = {i: self.db_ids[i] for i in range(len(self.db_ids))}\n",
    "        else:\n",
    "            dim = 1536  # default for 'text-embedding-ada-002'\n",
    "            print(\"No data found in DB. Initializing empty FAISS index.\")\n",
    "            self.index = faiss.IndexFlatIP(dim)\n",
    "            self.faiss_id_to_db_id = {}\n",
    "\n",
    "    def get_normalized_embedding(self, text: str):\n",
    "        \"\"\"\n",
    "        Embedding needs to be normalized to use cosine similarity via faiss\n",
    "        \"\"\"\n",
    "        response = self.client.embeddings.create(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            input=[text],\n",
    "        )\n",
    "        emb = np.array(response.data[0].embedding, dtype=\"float32\")\n",
    "        emb /= np.linalg.norm(emb)\n",
    "        return emb\n",
    "\n",
    "    def retrieve_top_k(self, query_log, k=3, score_threshold=0.8):\n",
    "        query_emb = self.get_normalized_embedding(query_log).astype(\"float32\").reshape(1, -1)\n",
    "\n",
    "        safe_k = min(k, self.index.ntotal)\n",
    "        # if k > self.index.ntotal:\n",
    "        #     print(f\"WARN: Only {self.index.ntotal} candidate examples available, returning top-{safe_k} examples\")\n",
    "        \n",
    "        similarities, indices = self.index.search(query_emb, k)\n",
    "\n",
    "        results = []\n",
    "        for sim, idx in zip(similarities[0], indices[0]):\n",
    "            if idx < 0:\n",
    "                continue\n",
    "            \n",
    "            db_id = self.faiss_id_to_db_id[idx]\n",
    "            record = None\n",
    "            if self.mode == \"full\":\n",
    "                record = self.db_records[db_id]\n",
    "            else:\n",
    "                self.c.execute(\"\"\"\n",
    "                SELECT log_line, template_generic, template_annotated, variables_json\n",
    "                FROM log_examples\n",
    "                WHERE id = ?\n",
    "                \"\"\", (db_id,))\n",
    "                row = self.c.fetchone()\n",
    "                if row:\n",
    "                    log_line, template_generic, template_annotated, variables_json = row\n",
    "                    record = {\n",
    "                        \"id\": db_id,\n",
    "                        \"original_log\": log_line,\n",
    "                        \"template_generic\": template_generic,\n",
    "                        \"template_annotated\": template_annotated,\n",
    "                        \"variables_json\": variables_json\n",
    "                    }\n",
    "                else:\n",
    "                    raise Exception(f\"Cannot find row with id {db_id}\")    \n",
    "            \n",
    "            if record:\n",
    "                record[\"similarity\"] = float(sim)\n",
    "                results.append(record)\n",
    "\n",
    "        if results:\n",
    "            top_score = results[0][\"similarity\"]\n",
    "            # print(f\"top score: {top_score}\")\n",
    "            if top_score < score_threshold:\n",
    "                print(f\"WARN: Top score similarity {top_score:.2f} below threshold {score_threshold}. Fallback to zero-shot.\")\n",
    "            \n",
    "        return results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "04f8217d-97fc-4ce1-b238-8af6a9568bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt creation\n",
    "\n",
    "PROMPT_REQUIREMENT = \"\"\"You are a log parser. For the given log line:\n",
    "1. Identify a generalized log template by replacing variable parts (timestamp, usernames, IPs, IDs, numbers, etc.) with clearly typed placeholders like <TIMESTAMP>, <USERNAME>, <IP_ADDRESS>.\n",
    "2. Extract the values of those variables in a JSON dictionary.\n",
    "3. Only output the packed JSON object (no triple backticks and no new line) with fields: \"template\", \"variables\", \"original_log\"\n",
    "4. The \"template\" field should be a plain string with <...> placeholders — **do not wrap the entire template with curly braces {}**.\n",
    "5. If variables have the same name, separate them with an underscore and number suffix.\"\"\"\n",
    "\n",
    "def create_prompt(log_line: str, few_shot_string: str = \"\") -> str:\n",
    "    if not log_line:\n",
    "        raise Exception(\"Cannot create prompt for empty log_line.\")\n",
    "    fse_prompt = f\"Few Shot Examples:\\n{few_shot_string}\" if few_shot_string else \"\"\n",
    "    ll_prompt = f\"Process the log line:\\n{log_line}\"\n",
    "    prompt = textwrap.dedent(f\"{PROMPT_REQUIREMENT}\\n\\n{fse_prompt}\\n\\n{ll_prompt}\")\n",
    "    # print(prompt)\n",
    "    return prompt\n",
    "\n",
    "def create_few_shot_examples_string(examples: list) -> str:\n",
    "    if not examples:\n",
    "        return \"\"\n",
    "    s = \"\"\n",
    "    for idx, e in enumerate(examples, start=1):\n",
    "        s += f\"\"\"\n",
    "        Example {idx}:\n",
    "        Log Line: {e[\"original_log\"]}\n",
    "        Template: {e[\"template_annotated\"]}\n",
    "        \"\"\"\n",
    "    return textwrap.dedent(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "118fffa5-fb19-43c3-90bc-9431b19fcc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key extraction function\n",
    "\n",
    "MODEL = \"gpt-4o\"\n",
    "\n",
    "class Extractor:\n",
    "    def __init__(self,\n",
    "                 log_retriever: LogRetriever = None,\n",
    "                 few_shot_enabled=True,\n",
    "                 k = 3,\n",
    "                 few_shot_threshold=0.0,\n",
    "                 debug=False):\n",
    "        self.client = lt.client\n",
    "        self.log_retriever = log_retriever\n",
    "        self.few_shot_enabled = few_shot_enabled\n",
    "        self.k = k\n",
    "        self.few_shot_threshold = few_shot_threshold\n",
    "        self.debug = debug\n",
    "        \n",
    "    def extract_log(self, log_line: str) -> dict:\n",
    "        few_shot_examples = self.log_retriever.retrieve_top_k(log_line, self.k, self.few_shot_threshold) if self.few_shot_enabled else \"\"\n",
    "        few_shot_string = create_few_shot_examples_string(few_shot_examples)\n",
    "        prompt = create_prompt(log_line, few_shot_string)\n",
    "        response = self.client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            temperature=0,\n",
    "            messages=[{\"role\": \"user\", \"content\":prompt}],\n",
    "        )\n",
    "        content = response.choices[0].message.content.strip()\n",
    "        if self.debug:\n",
    "            print(prompt)\n",
    "            print(content)\n",
    "        return content\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6235a314-717a-41cb-8555-e6b5dc1dbe2a",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667f2998-6930-40d3-bb8a-5a834ba1be26",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Zero-shot\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "lt = LogRetriever(db_path=DBNAME, mode=\"full\")\n",
    "ex = Extractor(log_retriever=lt, few_shot_enabled=False, debug=True)\n",
    "df = get_gold()\n",
    "df[\"Output\"] = None\n",
    "df[\"OutputTemplateGeneric\"] = None\n",
    "df[\"OutputTemplateAnnotated\"] = None\n",
    "df[\"OutputVariablesJson\"] = None\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Zero-shot\"):\n",
    "    output = ex.extract_log(row[\"OriginalLog\"])\n",
    "    df.loc[idx, \"Output\"] = output\n",
    "    try:\n",
    "        d = json.loads(output)\n",
    "        df.loc[idx, \"OutputTemplateGeneric\"] = create_generic_template(d[\"template\"])\n",
    "        df.loc[idx, \"OutputTemplateAnnotated\"] = d[\"template\"]\n",
    "        df.loc[idx, \"OutputVariablesJson\"] = json.dumps(d[\"variables\"])\n",
    "    except json.JSONDecodeError as e:\n",
    "        pass\n",
    "    \n",
    "    if idx % 10 == 0:\n",
    "        df.to_csv(\"zero_shot.csv\", index=False)\n",
    "\n",
    "# df.to_csv(\"zero_shot.csv\", index=False)\n",
    "print(\"Done with zero shot\")\n",
    "lt.close_conn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef19f67a-c1b0-479b-8eea-90f65abb0534",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Perfect few-shot learning with example of same template provided\n",
    "def insert_perfect_candidate_logs(log_retriever, gold_df):\n",
    "    candidate_log_ids = [74, 912, 78, 928, 1765, 3, 16, 292, 73, 1, 10, 1439, 12, 29]\n",
    "\n",
    "    candidate_set = []\n",
    "    for line_id in candidate_log_ids:\n",
    "        row = gold_df[gold_df['LineId'] == line_id]\n",
    "        if not row.empty:\n",
    "            row_dict = row.iloc[0].to_dict()\n",
    "            candidate_set.append({\n",
    "                \"log\": row_dict.get(\"OriginalLog\", \"\"),\n",
    "                \"at\": row_dict.get(\"TemplateAnnotated\", \"\"),\n",
    "                \"v\": row_dict.get(\"VariablesJson\", \"\"),\n",
    "                \"nl\" : log_retriever.get_normalized_embedding(row_dict.get(\"OriginalLog\", \"\"))\n",
    "            })\n",
    "\n",
    "    for candidate in candidate_set:\n",
    "        insert_log_example(\n",
    "            candidate.get(\"log\"),\n",
    "            candidate.get(\"at\"),\n",
    "            candidate.get(\"v\"),\n",
    "            candidate.get(\"nl\"),\n",
    "            log_retriever.conn\n",
    "        )\n",
    "        \n",
    "\n",
    "\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "\n",
    "df = get_gold()\n",
    "df[\"Output\"] = None\n",
    "df[\"OutputTemplateGeneric\"] = None\n",
    "df[\"OutputTemplateAnnotated\"] = None\n",
    "df[\"OutputVariablesJson\"] = None\n",
    "\n",
    "client = OpenAI() \n",
    "lt = LogRetriever(db_path=DBNAME, mode=\"full\")\n",
    "lt.truncate_log_examples_db()\n",
    "insert_perfect_candidate_logs(lt, df)\n",
    "lt.load_embeddings()\n",
    "ex = Extractor(log_retriever=lt, few_shot_enabled=True, k=3, few_shot_threshold = 0.0, debug=True)\n",
    "\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Perfect few-shot\"):\n",
    "    output = ex.extract_log(row[\"OriginalLog\"])\n",
    "    df.loc[idx, \"Output\"] = output\n",
    "    try:\n",
    "        d = json.loads(output)\n",
    "        df.loc[idx, \"OutputTemplateGeneric\"] = create_generic_template(d[\"template\"])\n",
    "        df.loc[idx, \"OutputTemplateAnnotated\"] = d[\"template\"]\n",
    "        df.loc[idx, \"OutputVariablesJson\"] = json.dumps(d[\"variables\"])\n",
    "    except json.JSONDecodeError as e:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    if idx % 10 == 0:\n",
    "        df.to_csv(\"perfect_few_shot.csv\", index=False)\n",
    "\n",
    "df.to_csv(\"perfect_few_shot.csv\", index=False)\n",
    "print(\"Done with perfect few shot\")\n",
    "lt.close_conn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281ee7b7-3334-458e-a573-027e114ace1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Partial few-shot learning with unseen test log templates\n",
    "def insert_partial_candidate_logs(log_retriever, gold_df):\n",
    "    # Template E1 - E10 in our test\n",
    "    # Will test on samples E11 - E14\n",
    "    candidate_log_ids = [74, 912, 78, 928, 1765, 3, 16, 292, 73]\n",
    "\n",
    "    candidate_set = []\n",
    "    for line_id in candidate_log_ids:\n",
    "        row = gold_df[gold_df['LineId'] == line_id]\n",
    "        if not row.empty:\n",
    "            row_dict = row.iloc[0].to_dict()\n",
    "            candidate_set.append({\n",
    "                \"log\": row_dict.get(\"OriginalLog\", \"\"),\n",
    "                \"at\": row_dict.get(\"TemplateAnnotated\", \"\"),\n",
    "                \"v\": row_dict.get(\"VariablesJson\", \"\"),\n",
    "                \"nl\" : log_retriever.get_normalized_embedding(row_dict.get(\"OriginalLog\", \"\"))\n",
    "            })\n",
    "\n",
    "    for candidate in candidate_set:\n",
    "        insert_log_example(\n",
    "            candidate.get(\"log\"),\n",
    "            candidate.get(\"at\"),\n",
    "            candidate.get(\"v\"),\n",
    "            candidate.get(\"nl\"),\n",
    "            log_retriever.conn\n",
    "        )\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "df = get_gold()\n",
    "df[\"Output\"] = None\n",
    "df[\"OutputTemplateGeneric\"] = None\n",
    "df[\"OutputTemplateAnnotated\"] = None\n",
    "df[\"OutputVariablesJson\"] = None\n",
    "\n",
    "client = OpenAI() \n",
    "lt = LogRetriever(db_path=DBNAME, mode=\"full\")\n",
    "lt.truncate_log_examples_db()\n",
    "insert_partial_candidate_logs(lt, df)\n",
    "lt.load_embeddings()\n",
    "ex = Extractor(log_retriever=lt, few_shot_enabled=True, k=3, few_shot_threshold = 0.0, debug=True)\n",
    "\n",
    "\n",
    "## Only test on log E11-E14\n",
    "ids_to_keep = [\"E11\", \"E12\", \"E13\", \"E14\"]\n",
    "df = df[df[\"EventId\"].isin(ids_to_keep)]\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Partial few-shot\"):\n",
    "    output = ex.extract_log(row[\"OriginalLog\"])\n",
    "    df.loc[idx, \"Output\"] = output\n",
    "    try:\n",
    "        d = json.loads(output)\n",
    "        df.loc[idx, \"OutputTemplateGeneric\"] = create_generic_template(d[\"template\"])\n",
    "        df.loc[idx, \"OutputTemplateAnnotated\"] = d[\"template\"]\n",
    "        df.loc[idx, \"OutputVariablesJson\"] = json.dumps(d[\"variables\"])\n",
    "    except json.JSONDecodeError as e:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    if idx % 10 == 0:\n",
    "        df.to_csv(\"partial_few_shot_e11_to_14.csv\", index=False)\n",
    "\n",
    "df.to_csv(\"partial_few_shot_e11_to_14.csv\", index=False)\n",
    "print(\"Done with partial few shot\")\n",
    "lt.close_conn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2473e0c3-fe67-46d6-bdb2-1c6b5aacbd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Partial few-shot learning with unseen test log templates\n",
    "## K = 1\n",
    "def insert_partial_candidate_logs(log_retriever, gold_df):\n",
    "    # Template E1 - E10 in our test\n",
    "    # Will test on samples E11 - E14\n",
    "    candidate_log_ids = [74, 912, 78, 928, 1765, 3, 16, 292, 73]\n",
    "\n",
    "    candidate_set = []\n",
    "    for line_id in candidate_log_ids:\n",
    "        row = gold_df[gold_df['LineId'] == line_id]\n",
    "        if not row.empty:\n",
    "            row_dict = row.iloc[0].to_dict()\n",
    "            candidate_set.append({\n",
    "                \"log\": row_dict.get(\"OriginalLog\", \"\"),\n",
    "                \"at\": row_dict.get(\"TemplateAnnotated\", \"\"),\n",
    "                \"v\": row_dict.get(\"VariablesJson\", \"\"),\n",
    "                \"nl\" : log_retriever.get_normalized_embedding(row_dict.get(\"OriginalLog\", \"\"))\n",
    "            })\n",
    "\n",
    "    for candidate in candidate_set:\n",
    "        insert_log_example(\n",
    "            candidate.get(\"log\"),\n",
    "            candidate.get(\"at\"),\n",
    "            candidate.get(\"v\"),\n",
    "            candidate.get(\"nl\"),\n",
    "            log_retriever.conn\n",
    "        )\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "df = get_gold()\n",
    "df[\"Output\"] = None\n",
    "df[\"OutputTemplateGeneric\"] = None\n",
    "df[\"OutputTemplateAnnotated\"] = None\n",
    "df[\"OutputVariablesJson\"] = None\n",
    "\n",
    "client = OpenAI() \n",
    "lt = LogRetriever(db_path=DBNAME, mode=\"full\")\n",
    "lt.truncate_log_examples_db()\n",
    "insert_partial_candidate_logs(lt, df)\n",
    "lt.load_embeddings()\n",
    "ex = Extractor(log_retriever=lt, few_shot_enabled=True, k=1, few_shot_threshold = 0.0, debug=True)\n",
    "\n",
    "\n",
    "## Only test on log E11-E14\n",
    "ids_to_keep = [\"E11\", \"E12\", \"E13\", \"E14\"]\n",
    "df = df[df[\"EventId\"].isin(ids_to_keep)]\n",
    "\n",
    "for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Partial few-shot\"):\n",
    "    output = ex.extract_log(row[\"OriginalLog\"])\n",
    "    df.loc[idx, \"Output\"] = output\n",
    "    try:\n",
    "        d = json.loads(output)\n",
    "        df.loc[idx, \"OutputTemplateGeneric\"] = create_generic_template(d[\"template\"])\n",
    "        df.loc[idx, \"OutputTemplateAnnotated\"] = d[\"template\"]\n",
    "        df.loc[idx, \"OutputVariablesJson\"] = json.dumps(d[\"variables\"])\n",
    "    except json.JSONDecodeError as e:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    if idx % 10 == 0:\n",
    "        df.to_csv(\"partial_few_shot_e11_to_14.csv\", index=False)\n",
    "\n",
    "df.to_csv(\"partial_few_shot_e11_to_14.csv\", index=False)\n",
    "print(\"Done with partial few shot\")\n",
    "lt.close_conn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e808a132-6781-440c-9733-dff942988e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Partial few-shot learning stratified log templates\n",
    "## K = 1\n",
    "def insert_one_out_candidate_logs(log_retriever, gold_df, exclude_index=None):\n",
    "    candidate_log_ids = [74, 912, 78, 928, 1765, 3, 16, 292, 73, 1, 10, 1439, 12, 29]\n",
    "    if exclude_index < len(candidate_log_ids) and exclude_index >= 0:\n",
    "        print(f\"Removed candidate log of type E{exclude_index+1}\")\n",
    "        candidate_log_ids.pop(exclude_index)\n",
    "    \n",
    "    candidate_set = []\n",
    "    for line_id in candidate_log_ids:\n",
    "        row = gold_df[gold_df['LineId'] == line_id]\n",
    "        if not row.empty:\n",
    "            row_dict = row.iloc[0].to_dict()\n",
    "            candidate_set.append({\n",
    "                \"log\": row_dict.get(\"OriginalLog\", \"\"),\n",
    "                \"at\": row_dict.get(\"TemplateAnnotated\", \"\"),\n",
    "                \"v\": row_dict.get(\"VariablesJson\", \"\"),\n",
    "                \"nl\" : log_retriever.get_normalized_embedding(row_dict.get(\"OriginalLog\", \"\"))\n",
    "            })\n",
    "\n",
    "    for candidate in candidate_set:\n",
    "        insert_log_example(\n",
    "            candidate.get(\"log\"),\n",
    "            candidate.get(\"at\"),\n",
    "            candidate.get(\"v\"),\n",
    "            candidate.get(\"nl\"),\n",
    "            log_retriever.conn\n",
    "        )\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "df = get_gold()\n",
    "df[\"Output\"] = None\n",
    "df[\"OutputTemplateGeneric\"] = None\n",
    "df[\"OutputTemplateAnnotated\"] = None\n",
    "df[\"OutputVariablesJson\"] = None\n",
    "\n",
    "client = OpenAI() \n",
    "lt = LogRetriever(db_path=DBNAME, mode=\"full\")\n",
    "\n",
    "for i in range(14):\n",
    "    event_id = f\"E{i+1}\"\n",
    "    print(f\"Working on {event_id}\")\n",
    "    lt.truncate_log_examples_db()\n",
    "    insert_one_out_candidate_logs(lt, df, i)\n",
    "    lt.load_embeddings()\n",
    "    ex = Extractor(log_retriever=lt, few_shot_enabled=True, k=1, few_shot_threshold = 0.0, debug=True)\n",
    "\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Partial few-shot\"):\n",
    "        if row[\"EventId\"] != event_id:\n",
    "            continue\n",
    "            \n",
    "        output = ex.extract_log(row[\"OriginalLog\"])\n",
    "        df.loc[idx, \"Output\"] = output\n",
    "        try:\n",
    "            d = json.loads(output)\n",
    "            df.loc[idx, \"OutputTemplateGeneric\"] = create_generic_template(d[\"template\"])\n",
    "            df.loc[idx, \"OutputTemplateAnnotated\"] = d[\"template\"]\n",
    "            df.loc[idx, \"OutputVariablesJson\"] = json.dumps(d[\"variables\"])\n",
    "        except json.JSONDecodeError as e:\n",
    "            pass\n",
    "        \n",
    "        \n",
    "        if idx % 10 == 0:\n",
    "            df.to_csv(\"partial_few_shot_leave_one_out.csv\", index=False)\n",
    "\n",
    "df.to_csv(\"partial_few_shot_leave_one_out.csv\", index=False)\n",
    "print(\"Done\")\n",
    "lt.close_conn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30020256-4bd4-40d7-9f6b-0f100cf97740",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8e5e8e-a039-42c4-8e83-69076879af83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_generic_vs_annotated(df: pd.DataFrame):\n",
    "    result = {}\n",
    "\n",
    "    for variant, gt_col, pred_col in [\n",
    "        (\"Generic\", \"TemplateGeneric\", \"OutputTemplateGeneric\"),\n",
    "        (\"Annotated\", \"TemplateAnnotated\", \"OutputTemplateAnnotated\"),\n",
    "    ]:\n",
    "        # Compute per-record parsing accuracy\n",
    "        df[f'ParsingAccuracy_{variant}'] = df[gt_col] == df[pred_col]\n",
    "        parsing_accuracy = df[f'ParsingAccuracy_{variant}'].mean()\n",
    "\n",
    "        # Compute Template Recall\n",
    "        event_group = df.groupby('EventId')[f'ParsingAccuracy_{variant}'].agg(lambda x: x.all())\n",
    "        template_recall = event_group.mean()\n",
    "\n",
    "        # Compute Template Precision\n",
    "        unique_pred_templates = df[pred_col].unique()\n",
    "        template_precision = event_group.sum() / len(unique_pred_templates)\n",
    "\n",
    "        result[variant] = {\n",
    "            \"Parsing Accuracy\": round(float(parsing_accuracy), 4),\n",
    "            \"Recall Template Accuracy\": round(float(template_recall), 4),\n",
    "            \"Precision Template Accuracy\": round(float(template_precision), 4),\n",
    "        }\n",
    "\n",
    "    # Display nicely\n",
    "    print(\"\\n=== ✅ Comparison: Generic vs Annotated ===\\n\")\n",
    "    print(f\"{'Metric':40} | {'Generic':8} | {'Annotated':8}\")\n",
    "    print(\"-\" * 65)\n",
    "    for metric in result['Generic'].keys():\n",
    "        generic_val = result['Generic'][metric]\n",
    "        annotated_val = result['Annotated'][metric]\n",
    "        print(f\"{metric:40} | {generic_val:<8} | {annotated_val:<8}\")\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def analyze_template_breakdown_generic(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Breaks down parsing errors to illustrate Recall and Precision issues.\n",
    "    Prints failed EventIds with mismatched records and fragmented output templates.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute per-record accuracy\n",
    "    df['ParsingAccuracyGeneric'] = df['TemplateGeneric'] == df['OutputTemplateGeneric']\n",
    "\n",
    "    # 1. Recall breakdown\n",
    "    event_group = df.groupby('EventId')['ParsingAccuracyGeneric'].agg(all_matched=lambda x: x.all())\n",
    "    failed_event_ids = event_group[event_group['all_matched'] == False].index.tolist()\n",
    "\n",
    "    print(\"\\n=== Recall Failures (EventIds with mismatched records) ===\")\n",
    "    for event_id in failed_event_ids:\n",
    "        group_df = df[df['EventId'] == event_id]\n",
    "        ground_truth_template = group_df['TemplateGeneric'].iloc[0]\n",
    "\n",
    "        print(f\"\\nEventId: {event_id}\")\n",
    "        print(f\"  Ground Truth Template: {ground_truth_template}\\n\")\n",
    "        \n",
    "        mismatches = group_df[~group_df['ParsingAccuracyGeneric']]\n",
    "        unique_wrong_preds = mismatches['OutputTemplateGeneric'].unique()\n",
    "\n",
    "        for i, pred in enumerate(unique_wrong_preds, 1):\n",
    "            print(f\"    Incorrect Pred {i}: {pred}\")\n",
    "\n",
    "    # 2. Precision breakdown\n",
    "    template_counts = df.groupby('EventId')['OutputTemplateGeneric'].nunique()\n",
    "    over_fragmented_event_ids = template_counts[template_counts > 1]\n",
    "\n",
    "    print(\"\\n=== Precision Issues (EventIds mapped to multiple templates) ===\")\n",
    "    for event_id in over_fragmented_event_ids.index:\n",
    "        templates = df[df['EventId'] == event_id]['OutputTemplateGeneric'].unique().tolist()\n",
    "        print(f\"\\nEventId: {event_id} generated {len(templates)} templates:\")\n",
    "        for i, t in enumerate(templates, 1):\n",
    "            print(f\"  {i}. {t}\")\n",
    "\n",
    "    return {\n",
    "        \"FailedEventIds_Recall\": failed_event_ids,\n",
    "        \"OverFragmentedEventIds_Precision\": over_fragmented_event_ids.to_dict()\n",
    "    }\n",
    "\n",
    "def analyze_template_breakdown_annotated(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Breaks down parsing errors for annotated templates.\n",
    "    Uses fields: TemplateAnnotated (GT) and OutputTemplateAnnotated (Prediction).\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute per-record parsing accuracy for annotated templates\n",
    "    df['ParsingAccuracyAnnotated'] = df['TemplateAnnotated'] == df['OutputTemplateAnnotated']\n",
    "\n",
    "    # 1. Recall breakdown (EventIds where not all rows matched)\n",
    "    event_group = df.groupby('EventId')['ParsingAccuracyAnnotated'].agg(all_matched=lambda x: x.all())\n",
    "    failed_event_ids = event_group[event_group['all_matched'] == False].index.tolist()\n",
    "\n",
    "    print(\"\\n=== Recall Failures (EventIds with mismatched records) ===\")\n",
    "    for event_id in failed_event_ids:\n",
    "        group_df = df[df['EventId'] == event_id]\n",
    "        ground_truth_template = group_df['TemplateAnnotated'].iloc[0]\n",
    "\n",
    "        print(f\"\\nEventId: {event_id}\")\n",
    "        print(f\"  Ground Truth Template: {ground_truth_template}\\n\")\n",
    "        \n",
    "        mismatches = group_df[~group_df['ParsingAccuracyAnnotated']]\n",
    "        unique_wrong_preds = mismatches['OutputTemplateAnnotated'].unique()\n",
    "\n",
    "        for i, pred in enumerate(unique_wrong_preds, 1):\n",
    "            print(f\"    Predicted {i}: {pred}\")\n",
    "\n",
    "    # 2. Precision breakdown (EventIds that map to multiple predicted templates)\n",
    "    template_counts = df.groupby('EventId')['OutputTemplateAnnotated'].nunique()\n",
    "    over_fragmented_event_ids = template_counts[template_counts > 1]\n",
    "\n",
    "    print(\"\\n=== Precision Issues (EventIds mapped to multiple templates) ===\")\n",
    "    for event_id in over_fragmented_event_ids.index:\n",
    "        templates = df[df['EventId'] == event_id]['OutputTemplateAnnotated'].unique().tolist()\n",
    "        print(f\"\\nEventId: {event_id} generated {len(templates)} templates:\")\n",
    "        for i, t in enumerate(templates, 1):\n",
    "            print(f\"  {i}. {t}\")\n",
    "\n",
    "    return {\n",
    "        \"FailedEventIds_Recall\": failed_event_ids,\n",
    "        \"OverFragmentedEventIds_Precision\": over_fragmented_event_ids.to_dict()\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "ce7f9660-c446-4b8b-8044-29c0ed625b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "ZERO_SHOT_OUTPUT = \"zero_shot_final.csv\"\n",
    "PERFECT_FEW_SHOT_OUTPUT = \"perfect_few_shot_final.csv\"\n",
    "PARTIAL_FEW_SHOT_LEAVE_ONE_OUT = \"partial_few_shot_leave_one_out_final.csv\"\n",
    "\n",
    "zero_df = pd.read_csv(ZERO_SHOT_OUTPUT)\n",
    "perfect_df = pd.read_csv(PERFECT_FEW_SHOT_OUTPUT)\n",
    "partial_df = pd.read_csv(PARTIAL_FEW_SHOT_LEAVE_ONE_OUT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "b723ea29-8435-4c92-931d-64dcddd1e66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Recall Failures (EventIds with mismatched records) ===\n",
      "\n",
      "EventId: E1\n",
      "  Ground Truth Template: <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: <IP_ADDRESS_1>:<PORT> Served block blk_<BLOCK_ID> to /<IP_ADDRESS_2>\n",
      "\n",
      "    Predicted 1: <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: <IP_ADDRESS>:<PORT> Served block blk_<BLOCK_ID> to /<CLIENT_IP_ADDRESS>\n",
      "    Predicted 2: <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: <IP_ADDRESS>:<PORT> Served block blk_<BLOCK_ID> to /<DEST_IP_ADDRESS>\n",
      "    Predicted 3: <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: <IP_ADDRESS_1>:<PORT_1> Served block blk_<BLOCK_ID> to /<IP_ADDRESS_2>\n",
      "\n",
      "EventId: E10\n",
      "  Ground Truth Template: <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: PacketResponder <PACKET_RESPONDER_NUM> for block blk_<BLOCK_ID> terminating\n",
      "\n",
      "    Predicted 1: <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: PacketResponder <RESPONDER_ID> for block blk_<BLOCK_ID> terminating\n",
      "    Predicted 2: <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: PacketResponder <NUMBER> for block blk_<BLOCK_ID> terminating\n",
      "\n",
      "EventId: E11\n",
      "  Ground Truth Template: <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: Received block blk_<BLOCK_ID> of size <SIZE> from /<IP_ADDRESS>\n",
      "\n",
      "    Predicted 1: <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: Received block blk_<BLOCK_ID> of size <SIZE> from /<SRC_IP_ADDRESS>\n",
      "    Predicted 2: <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: Received block blk_<BLOCK_ID> of size <BLOCK_SIZE> from /<IP_ADDRESS>\n",
      "\n",
      "EventId: E2\n",
      "  Ground Truth Template: <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: <IP_ADDRESS_1>:<PORT_1> Starting thread to transfer block blk_<BLOCK_ID> to <IP_ADDRESS_2>:<PORT_2>\n",
      "\n",
      "    Predicted 1: <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: <SRC_IP_ADDRESS>:<SRC_PORT> Starting thread to transfer block blk_<BLOCK_ID> to <DEST_IP_ADDRESS>:<DEST_PORT>\n",
      "\n",
      "EventId: E3\n",
      "  Ground Truth Template: <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: <IP_ADDRESS_1>:<PORT>:Got exception while serving blk_<BLOCK_ID> to /<IP_ADDRESS_2>:\n",
      "\n",
      "    Predicted 1: <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: <IP_ADDRESS_1>:<PORT>:Got exception while serving blk_<BLOCK_ID> to /<IP_ADDRESS_2>\n",
      "\n",
      "EventId: E4\n",
      "  Ground Truth Template: <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* ask <IP_ADDRESS>:<PORT> to delete  blk_<BLOCK_ID>\n",
      "\n",
      "    Predicted 1: <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* ask <IP_ADDRESS>:<PORT> to delete blk_<BLOCK_ID>\n",
      "\n",
      "EventId: E5\n",
      "  Ground Truth Template: <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* ask <IP_ADDRESS_1>:<PORT_1> to replicate blk_<BLOCK_ID> to datanode(s) <IP_ADDRESS_2>:<PORT_2>\n",
      "\n",
      "    Predicted 1: <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* ask <IP_ADDRESS>:<PORT> to replicate blk_<BLOCK_ID> to datanode(s) <IP_ADDRESS_2>:<PORT_2>\n",
      "\n",
      "EventId: E7\n",
      "  Ground Truth Template: <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: /<FILE_PATH>/part-<PART_ID>. blk_<BLOCK_ID>\n",
      "\n",
      "    Predicted 1: <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: <FILE_PATH> blk_<BLOCK_ID>\n",
      "    Predicted 2: <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: <FILE_PATH>. blk_<BLOCK_ID>\n",
      "    Predicted 3: <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: /user/root/sortrand/_temporary/_task_<TASK_ID>/part-<PART_ID>. blk_<BLOCK_ID>\n",
      "    Predicted 4: <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: /user/root/sortrand/_temporary/_task_<TASK_ID>_r_<TASK_ID_2>/part-<PART_ID>. blk_<BLOCK_ID>\n",
      "    Predicted 5: <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: <PATH>. blk_<BLOCK_ID>\n",
      "    Predicted 6: <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: /user/root/randtxt/_temporary/_task_<TASK_ID>_m_<MAP_ID>_<PART_ID>/part-<PART_NUMBER>. blk_<BLOCK_ID>\n",
      "    Predicted 7: <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: /user/root/randtxt/_temporary/_task_<TASK_ID>_<JOB_ID>_<MAP_ID>_<PART_ID>. blk_<BLOCK_ID>\n",
      "    Predicted 8: <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: /user/root/rand/_temporary/_task_<TASK_ID>_<JOB_ID>_<MAP_ID>_<PART_ID>/part-<PART_NUMBER>. blk_<BLOCK_ID>\n",
      "    Predicted 9: <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: /user/root/rand/_temporary/_task_<TASK_ID>_<JOB_ID>_<TASK_TYPE>_<TASK_NUMBER>_<PART_NUMBER>/part-<PART_NUMBER>. blk_<BLOCK_ID>\n",
      "    Predicted 10: <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: /user/root/rand/_temporary/_task_<TASK_ID>_m_<TASK_NUMBER>_<PART_NUMBER>/part-<PART_ID>. blk_<BLOCK_ID>\n",
      "    Predicted 11: <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: /user/<USERNAME>/<DIR>/<TEMPORARY>/<TASK>/<TASK_ID>/<PART>. blk_<BLOCK_ID>\n",
      "    Predicted 12: <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: /user/root/rand/_temporary/_task_<TASK_ID>/part-<PART_ID>. blk_<BLOCK_ID>\n",
      "    Predicted 13: <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: <PATH> blk_<BLOCK_ID>\n",
      "    Predicted 14: <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: /user/root/rand/_temporary/_task_<TASK_ID>_m_<MAP_ID>_<ID>/part-<PART_ID>. blk_<BLOCK_ID>\n",
      "    Predicted 15: <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: /user/root/randtxt2/_temporary/_task_<TASK_ID>_m_<MAP_ID>_0/part-<PART_ID>. blk_<BLOCK_ID>\n",
      "    Predicted 16: <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: /user/root/rand4/_temporary/_task_<TASK_ID>/part-<PART_ID>. blk_<BLOCK_ID>\n",
      "    Predicted 17: <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: /user/root/rand4/_temporary/_task_<TASK_ID>_<JOB_ID>_<MAP_ID>_<PART_ID>/part-<PART_NUMBER>. blk_<BLOCK_ID>\n",
      "    Predicted 18: <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: /user/root/rand6/_temporary/_task_<TASK_ID>_m_<MAP_ID>_0/part-<PART_ID>. blk_<BLOCK_ID>\n",
      "    Predicted 19: <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: /user/root/rand6/_temporary/_task_<TASK_ID>_<JOB_ID>_<MAP_ID>_<ATTEMPT_ID>/part-<PART_ID>. blk_<BLOCK_ID>\n",
      "    Predicted 20: <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: /user/root/rand7/_temporary/_task_<TASK_ID>/part-<PART_ID>. blk_<BLOCK_ID>\n",
      "    Predicted 21: <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: /user/root/rand8/_temporary/_task_<TASK_ID>/part-<PART_ID>. blk_<BLOCK_ID>\n",
      "    Predicted 22: <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: /user/root/rand8/_temporary/_task_<TASK_ID>_m_<MAP_ID>/part-<PART_ID>. blk_<BLOCK_ID>\n",
      "    Predicted 23: <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: /user/root/rand8/_temporary/_task_<TASK_ID>_m_<MAP_ID>_<ID>_<PART_ID>. blk_<BLOCK_ID>\n",
      "    Predicted 24: <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: /user/root/rand8/_temporary/_task_<TASK_ID>_<JOB_ID>_<MAP_ID>_<PART_ID>/part-<PART_NUMBER>. blk_<BLOCK_ID>\n",
      "    Predicted 25: <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: /user/root/randtxt9/_temporary/_task_<TASK_ID>_<JOB_ID>_m_<MAP_ID>_<REDUCE_ID>/part-<PART_ID>. blk_<BLOCK_ID>\n",
      "\n",
      "EventId: E9\n",
      "  Ground Truth Template: <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: Deleting block blk_<BLOCK_ID_1> file /<FILE_PATH>/blk_<BLOCK_ID_2>\n",
      "\n",
      "    Predicted 1: <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: Deleting block blk_<BLOCK_ID> file <FILE_PATH>\n",
      "\n",
      "=== Precision Issues (EventIds mapped to multiple templates) ===\n",
      "\n",
      "EventId: E1 generated 4 templates:\n",
      "  1. <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: <IP_ADDRESS>:<PORT> Served block blk_<BLOCK_ID> to /<CLIENT_IP_ADDRESS>\n",
      "  2. <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: <IP_ADDRESS>:<PORT> Served block blk_<BLOCK_ID> to /<DEST_IP_ADDRESS>\n",
      "  3. <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: <IP_ADDRESS_1>:<PORT> Served block blk_<BLOCK_ID> to /<IP_ADDRESS_2>\n",
      "  4. <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: <IP_ADDRESS_1>:<PORT_1> Served block blk_<BLOCK_ID> to /<IP_ADDRESS_2>\n",
      "\n",
      "EventId: E10 generated 2 templates:\n",
      "  1. <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: PacketResponder <RESPONDER_ID> for block blk_<BLOCK_ID> terminating\n",
      "  2. <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: PacketResponder <NUMBER> for block blk_<BLOCK_ID> terminating\n",
      "\n",
      "EventId: E11 generated 3 templates:\n",
      "  1. <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: Received block blk_<BLOCK_ID> of size <SIZE> from /<SRC_IP_ADDRESS>\n",
      "  2. <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: Received block blk_<BLOCK_ID> of size <BLOCK_SIZE> from /<IP_ADDRESS>\n",
      "  3. <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: Received block blk_<BLOCK_ID> of size <SIZE> from /<IP_ADDRESS>\n",
      "\n",
      "EventId: E3 generated 2 templates:\n",
      "  1. <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: <IP_ADDRESS_1>:<PORT>:Got exception while serving blk_<BLOCK_ID> to /<IP_ADDRESS_2>:\n",
      "  2. <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: <IP_ADDRESS_1>:<PORT>:Got exception while serving blk_<BLOCK_ID> to /<IP_ADDRESS_2>\n",
      "\n",
      "EventId: E7 generated 25 templates:\n",
      "  1. <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: <FILE_PATH> blk_<BLOCK_ID>\n",
      "  2. <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: <FILE_PATH>. blk_<BLOCK_ID>\n",
      "  3. <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: /user/root/sortrand/_temporary/_task_<TASK_ID>/part-<PART_ID>. blk_<BLOCK_ID>\n",
      "  4. <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: /user/root/sortrand/_temporary/_task_<TASK_ID>_r_<TASK_ID_2>/part-<PART_ID>. blk_<BLOCK_ID>\n",
      "  5. <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: <PATH>. blk_<BLOCK_ID>\n",
      "  6. <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: /user/root/randtxt/_temporary/_task_<TASK_ID>_m_<MAP_ID>_<PART_ID>/part-<PART_NUMBER>. blk_<BLOCK_ID>\n",
      "  7. <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: /user/root/randtxt/_temporary/_task_<TASK_ID>_<JOB_ID>_<MAP_ID>_<PART_ID>. blk_<BLOCK_ID>\n",
      "  8. <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: /user/root/rand/_temporary/_task_<TASK_ID>_<JOB_ID>_<MAP_ID>_<PART_ID>/part-<PART_NUMBER>. blk_<BLOCK_ID>\n",
      "  9. <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: /user/root/rand/_temporary/_task_<TASK_ID>_<JOB_ID>_<TASK_TYPE>_<TASK_NUMBER>_<PART_NUMBER>/part-<PART_NUMBER>. blk_<BLOCK_ID>\n",
      "  10. <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: /user/root/rand/_temporary/_task_<TASK_ID>_m_<TASK_NUMBER>_<PART_NUMBER>/part-<PART_ID>. blk_<BLOCK_ID>\n",
      "  11. <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: /user/<USERNAME>/<DIR>/<TEMPORARY>/<TASK>/<TASK_ID>/<PART>. blk_<BLOCK_ID>\n",
      "  12. <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: /user/root/rand/_temporary/_task_<TASK_ID>/part-<PART_ID>. blk_<BLOCK_ID>\n",
      "  13. <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: <PATH> blk_<BLOCK_ID>\n",
      "  14. <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: /user/root/rand/_temporary/_task_<TASK_ID>_m_<MAP_ID>_<ID>/part-<PART_ID>. blk_<BLOCK_ID>\n",
      "  15. <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: /user/root/randtxt2/_temporary/_task_<TASK_ID>_m_<MAP_ID>_0/part-<PART_ID>. blk_<BLOCK_ID>\n",
      "  16. <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: /user/root/rand4/_temporary/_task_<TASK_ID>/part-<PART_ID>. blk_<BLOCK_ID>\n",
      "  17. <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: /user/root/rand4/_temporary/_task_<TASK_ID>_<JOB_ID>_<MAP_ID>_<PART_ID>/part-<PART_NUMBER>. blk_<BLOCK_ID>\n",
      "  18. <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: /user/root/rand6/_temporary/_task_<TASK_ID>_m_<MAP_ID>_0/part-<PART_ID>. blk_<BLOCK_ID>\n",
      "  19. <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: /user/root/rand6/_temporary/_task_<TASK_ID>_<JOB_ID>_<MAP_ID>_<ATTEMPT_ID>/part-<PART_ID>. blk_<BLOCK_ID>\n",
      "  20. <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: /user/root/rand7/_temporary/_task_<TASK_ID>/part-<PART_ID>. blk_<BLOCK_ID>\n",
      "  21. <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: /user/root/rand8/_temporary/_task_<TASK_ID>/part-<PART_ID>. blk_<BLOCK_ID>\n",
      "  22. <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: /user/root/rand8/_temporary/_task_<TASK_ID>_m_<MAP_ID>/part-<PART_ID>. blk_<BLOCK_ID>\n",
      "  23. <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: /user/root/rand8/_temporary/_task_<TASK_ID>_m_<MAP_ID>_<ID>_<PART_ID>. blk_<BLOCK_ID>\n",
      "  24. <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: /user/root/rand8/_temporary/_task_<TASK_ID>_<JOB_ID>_<MAP_ID>_<PART_ID>/part-<PART_NUMBER>. blk_<BLOCK_ID>\n",
      "  25. <DATE> <TIME> <PID> <LOG_LEVEL> <COMPONENT>: BLOCK* NameSystem.allocateBlock: /user/root/randtxt9/_temporary/_task_<TASK_ID>_<JOB_ID>_m_<MAP_ID>_<REDUCE_ID>/part-<PART_ID>. blk_<BLOCK_ID>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'FailedEventIds_Recall': ['E1',\n",
       "  'E10',\n",
       "  'E11',\n",
       "  'E2',\n",
       "  'E3',\n",
       "  'E4',\n",
       "  'E5',\n",
       "  'E7',\n",
       "  'E9'],\n",
       " 'OverFragmentedEventIds_Precision': {'E1': 4,\n",
       "  'E10': 2,\n",
       "  'E11': 3,\n",
       "  'E3': 2,\n",
       "  'E7': 25}}"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "To analyze, use these functions:\n",
    "\n",
    "compare_generic_vs_annotated(df)\n",
    "analyze_template_breakdown_generic(df)\n",
    "analyze_template_breakdown_annotated(df)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
